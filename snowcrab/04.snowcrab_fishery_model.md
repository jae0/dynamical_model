
# Dynamical models for snow crab fisheries in the Maritimes Region of Canada, implemented in Julia

---
 

## The main modelling tools used:
  
  * [Julia](https://julialang.org/) 
  * [Turing](https://turing.ml/stable/)
  * [SciML](https://docs.sciml.ai/Overview/stable/)
  * [DifferentialEquations](https://github.com/SciML/DifferentialEquations.jl)
  * Other libraries used can be found in the set up environment (*.environment.jl) for each project

---

## Data requirements: 

If data files have not already been created, then do so. Data files are generated by either:

  - [bio.snowcrab::03.snowcrab_index_carstm.r](https://github.com/jae0/bio.snowcrab/blob/f2e0722de0bfa08611caa775cf6d912807bd4d91/inst/scripts/03.biomass_index_sizestructured_carstm.r)

or 

  - [bio.snowcrab::03.snowcrab_index_sizestructured_carstm.r](https://github.com/jae0/bio.snowcrab/blob/f2e0722de0bfa08611caa775cf6d912807bd4d91/inst/scripts/03.snowcrab_index_sizestructured_carstm.r)  

and then copy the files manually to appropriate locations or run:

```r

# R-code snippet show data formatting step

source( file.path( code_root, "bio_startup.R" )  )
loadfunctions("bio.snowcrab")
year.assessment = 2023

# choose correct location of input data
save_location = file.path( homedir, "bio.data", "bio.snowcrab", "modelled", "1999_present_fb", "fishery_model_results", "turing1"  )
save_location = file.path( homedir, "bio.data", "bio.snowcrab", "modelled" )
save_location = file.path( homedir, "projects", "dynamical_model", "snowcrab", "data" )

# prep data for discrete version
if (grepl("logistic_discrete", model_variation )) {
  fishery_model_data_inputs( year.assessment=year.assessment, type="biomass_dynamics", for_julia=TRUE, save_location=save_location  )
}

# prep data for continuous version: 
if (grepl("size_structured", model_variation)) {
    # fishery landings has a weekly time step = 2/52 ~ 0.0385 ~ 0.04  X dt=0.01 seems to work best
    fishery_model_data_inputs( year.assessment=year.assessment, type="size_structured_numerical_dynamics", for_julia=TRUE, time_resolution=2/52, save_location=save_location   )
}

```

The remaining code are in Julia.


---

## Define key directories and main run level options: 

These directories need to be defined. 

```julia

# choose correct directories
runtype = "operational"
runtype = "development"


if runtype == "operational"
  project_directory = joinpath( homedir(), "bio", "bio.snowcrab", "inst", "julia" ) 
  outputs_directory = joinpath( homedir(), "bio.data", "bio.snowcrab", "fishery_model" ) 
 
elseif runtype == "development"
  project_directory = joinpath( homedir(), "projects", "dynamical_model", "snowcrab" ) 
  outputs_directory = joinpath( project_directory, "outputs" ) 
end
 

# choose
bio_data_directory = joinpath( homedir(), "bio.data", "bio.snowcrab", "modelled", "1999_present_fb", "fishery_model_results", "turing1"  )
bio_data_directory = joinpath( project_directory, "data" )  



# time
year_assessment = 2023   # <<<<<<<<-- change

yrs = 1999:year_assessment  

# model-type
model_variation = "size_structured_dde_normalized" 
# model_variation = "logistic_discrete_historical"   
# model_variation = "logistic_discrete_basic"     

  #= 
    currently implemented models:
    
    model_variation = "logistic_discrete_historical"    #   pre-2022 method  :: ~ 1 hr # pre-2022, no normalization, q-based observation model
    model_variation = "logistic_discrete_basic"     # q catchability only for observation model
    model_variation = "logistic_discrete"              # q and intercept for observation model
    model_variation = "logistic_discrete_map"  # logistic map ... more extreme fluctuations
    model_variation = "size_structured_dde_normalized"  #      24hrs, >24 hrs
    model_variation = "size_structured_dde_unnormalized"  # basic continuous model without normalization ... very very slow .. do not use (incomplete params need tweaking) ::   24hrs, >24 hrs
  =#

# area to model .. choose a region of interest:
aulab ="cfanorth"     # 41 hrs (size struct), 20 min (logistic_discrete_hist)
aulab ="cfasouth"     # 65 hrs (size struct), min (logistic_discrete_hist)  
aulab ="cfa4x"        # 24 hrs (size struct), min (logistic_discrete_hist)


```

  
---

## Define global variables and model-specific save location and test

```julia  

include( joinpath(project_directory, "snowcrab_startup.jl" ) )  # add some paths and package requirements

# temp fix:
include( joinpath(project_directory, "bijectors_override.jl" ) )  

# above instantiates model as fmod: make sure fmod works
rand(fmod) 


   
## Testing and initial run parameters -- can safely skip over

#=
  # debugging/development: test dynamical model with generic/random parameters
  (test, pl) = fishery_model_test( "basic" ); pl
  (test, pl) = fishery_model_test( "random_external_forcing"  ); pl
  (test, pl) = fishery_model_test( "fishing"  ); pl
  (test, pl) = fishery_model_test( "nofishing" ); pl
  showall( summarize( test ) )
  
  # Choose AD backend -- (default) forwarddiff is stable and best choice at the moment
  using ForwardDiff
  using Turing
  
  using SciMLSensitivity
  using ReverseDiff # slow
  
  HMC(0.1, 5; adtype=AutoForwardDiff(; chunksize=0))
  
  # must re-initialize models if redefining params:
  # fmod = size_structured_dde_turing( PM=PM, solver_params=solver_params )
  # fmod = logistic_discrete_turing_historical( PM )  # q only

  using Symbolics
  du0 = copy(u0)
  jac_sparsity = Symbolics.jacobian_sparsity(
    (du, u) -> size_structured_dde!( du, u, h, p, 0.0),
    du0, u0
  )

  # KenCarp47(linsolve = LinearSolve.KrylovJL_GMRES())

=#
```

# ---
## Sample from posteriors after defining sampler  

Determine Turing run parameters that work reasonably and obtain posterior samples of parameter estimates.



```julia

# choose a sampler and associated sampler parameters (most are already defined in environments )

# Default is to use NUTS as defined in the environment. Change as required. E.g. use SMC for delay differential models due to speed of NUTS.


#=

    # D1. for debugging: NUTS requires RAM 80 GB .... and  slow
    # use this for logistic* models (discrete)  
    n_adapts, n_samples, n_chains, target_acceptance_rate, max_depth, init_ϵ = 10, 10, 1, 0.65, 7, 0.01
 
    # D2. for debugging: SMC is fast, reasonable mixing
    # use this for  size_structured* (continuous)
    n_samples, n_chains, thinning = 1000, 1, 10 # 1/2 will be dropped as warmup
 
    # P1. for production: NUTS-specific: 100GB RAM  for delay differential model .. memory leak? 
    n_adapts, n_samples, n_chains,  = 500, 500, 4, 
 
    # P2. for production: SMC:
    n_samples, n_chains, thinning = 10000, 4, 5 # for logistic 
 
    # test with SMC as it is fast and provides behavioural range
    # ensure basic solutions are within range .. testing balance of parameter effects (positive, follows data, etc)
    # then choose NUTS  (if it is not too slow)
 
    turing_sampler = Turing.NUTS(n_adapts, target_acceptance_rate; max_depth=max_depth, init_ϵ=init_ϵ ) 
 
    turing_sampler = Turing.SMC()
    
    turing_sampler = Turing.SGLD()   # Stochastic Gradient Langevin Dynamics (SGLD); slow, mixes poorly
    turing_sampler = Turing.HMC(0.01, 7)  # very slow; no mixings
    turing_sampler = Turing.HMCDA(0.25, 0.65)  #  total leapfrog length, target accept ratio.
    turing_sampler = Turing.NUTS{Turing.ReverseDiffAD{true}}( n_adapts, 0.65 ) # , init_ϵ=0.001
    turing_sampler = Turing.NUTS( 0.65 ) # , init_ϵ=0.001
    turing_sampler = Turing.DynamicNUTS()   # requires DynamicHMC
    
    # to do Variational Inference (an sd term goes less than 0 .. not sure of the cause ):
    using DistributionsAD, AdvancedVI
    samples_per_step, max_iters = 10, 1000  # Number of samples used to estimate the ELBO in each optimization step.
    res_vi =  vi(fmod, Turing.ADVI( samples_per_step, max_iters)); 
    res_vi_samples = rand( res_vi, 1000)  # sample via simulation


    # to over-ride solver params some work better with different samplers (over-writing lists is convoluted ... ):
    solver_params = @set solver_params.solver = MethodOfSteps(Rodas5())  # default safer
    solver_params = @set solver_params.solver = MethodOfSteps(KenCarp47())  # safe
    solver_params = @set solver_params.solver = MethodOfSteps(AutoTsit5(Rodas5()))  # automatic switching between stiff and non-stiff dynamics  
    solver_params = @set solver_params.solver = MethodOfSteps(AutoTsit5(KenCarp47()))  # faster 

    # to reload environment:
    include( joinpath(project_directory, "snowcrab_startup.jl" ) )  # add some paths and package requirements

=#


# MCMC samples
  
  # debug in serial model: 
  if false
      # model_variation = "logistic_discrete_historical" # unnormalized
      # model_variation = "logistic_discrete_basic"      # normalized 
      
      # aulab ="cfanorth"     # 41 hrs (size struct), 20 min (logistic_discrete_hist)
      # aulab ="cfasouth"     # 65 hrs (size struct), min (logistic_discrete_hist)  
      # aulab ="cfa4x"        # 24 hrs (size struct), min (logistic_discrete_hist)

      include( joinpath(project_directory, "snowcrab_startup.jl" ) )  
 
      res = res0 = sample( fmod, Turing.SMC(), 1000; init_ϵ=init_ϵ, drop_warmup=true, progress=true)

      res = sample( fmod, Turing.NUTS(), 100; init_ϵ=init_ϵ, drop_warmup=true, progress=true)
 
      showall( summarize( res ) )
 
      m, num, bio, trace, trace_bio, trace_time = fishery_model_predictions(res ) 
      Fkt, FR, FM = fishery_model_mortality()   # fishing (kt), relative Fishing mortality, instantaneous fishing mortality:
      pl = fishery_model_plot( toplot=("survey", "fishing" ) )
      # pl = plot(pl, ylim=(0, 4.0))
      pl = fishery_model_plot( toplot="fishing_mortality" )
      # pl = plot(pl, ylim=(0, 0.65))
      pl = fishery_model_plot( toplot="harvest_control_rule", n_sample=1000 ) #, alphav=0.01 )  # hcr
  
  end


# production mode in parallel
Random.seed!(year_assessment);

Logging.disable_logging(Logging.Warn) # or e.g. Logging.Info
res = sample( fmod, turing_sampler, MCMCThreads(), n_samples, n_chains ; init_ϵ=init_ϵ, drop_warmup=true, progress=true ) # sample in parallel

# .. make sure to check autocorrelation where required and thin
 

#=    
  # Alternatives: 

    # Restart from pre-determined means

    # find some functional inits
    res0 = sample( fmod, Turing.SMC(), 1000;  drop_warmup=true, progress=true)

    res  =  sample( 
      fmod, turing_sampler, MCMCThreads(), n_samples, n_chains;
      init_ϵ=init_ϵ, drop_warmup=true, progress=true, verbose=true,
      init_params=FillArrays.Fill(summarize(res0).nt[2], n_chains)
    )

    #  thinning=thinning,  ## thinning causes errors  not sure why 07/23
      
  
    # for testing: extract and reformat results
    # n scaled, n unscaled, biomass of fb with and without fishing, model_traces, model_times 
    m, num, bio, trace, trace_bio, trace_time = fishery_model_predictions(res  )

    # fishing (kt), relative Fishing mortlaity, instantaneous fishing mortality:
    Fkt, FR, FM = fishery_model_mortality() 
    pl = fishery_model_plot( toplot=("survey", "fishing",  "nofishing", "trace") )
    #  pl = plot(pl, ylim=(0, 70))

    pl = fishery_model_plot( toplot="harvest_control_rule" )  # hcr with fishing footprint

    showall( summarize( res ) )
  
    describe(res)
 
    summarystats(res)
    summarystats(res[:,1:4,:])

    plot( autocorplot(res) )
    plot(res)
    plot( traceplot(res) )
    plot( meanplot(res) )
    plot( density(res) )
    plot( histogram(res) )
    plot( mixeddensity(res) )
    #  pl = plot(pl, ylim=(0, 0.65))

=#

```


---

## Save results:

Using model solutions, compute other variables of interest via posterior simulation and load into main memory:

```julia
  # save results to (model_outdir) as a hdf5  .. can also read back in R as:  h5read( res_fn, "res" )
  # directory location is created in environment
  # res_fn = joinpath( model_outdir, string("results_turing", "_", aulab, ".hdf5" ) )   #defined in *.environment.jl
  @save res_fn res

  #=  to reload a save file:

    @load res_fn res
  
  =#
   
  # generate predicted trajectories and params of interest from posterior samples 
  # n scaled, n unscaled, biomass of fb with and without fishing, model_traces, model_times 
  m, num, bio, trace, trace_bio, trace_time = fishery_model_predictions(res ) 
  Fkt, FR, FM = fishery_model_mortality()   # fishing (kt), relative Fishing mortality, instantaneous fishing mortality:

  # quick check of fits and solutions:
  # fishery_model_plot( toplot=("survey", "fishing"), alphav=0.025 )


  # save a few data files as semicolon-delimited CSV's for use outside Julia
  summary_fn = joinpath( model_outdir, string("results_turing", "_", aulab, "_summary", ".csv" ) )  
  CSV.write( summary_fn,  summarize( res ), delim=";" )  # use semicolon as , also used in parm names
  
  if  occursin( r"logistic_discrete", model_variation ) 
    bio_fn1 = joinpath( model_outdir, string("results_turing", "_", aulab, "_bio_fishing", ".csv" ) )  
    CSV.write( bio_fn1,  DataFrame(bio[:,:], :auto), delim=";" )  # use semicolon as , also used in parm names
  end

  if  occursin( r"size_structured", model_variation ) 
    bio_fn1 = joinpath( model_outdir, string("results_turing", "_", aulab, "_bio_fishing", ".csv" ) )  
    CSV.write( bio_fn1,  DataFrame(bio[:,:,1], :auto), delim=";" )  # use semicolon as , also used in parm names

    bio_fn2 = joinpath( model_outdir, string("results_turing", "_", aulab, "_bio_nofishing", ".csv" ) )  
    CSV.write( bio_fn2,  DataFrame( bio[:,:,2], :auto), delim=";" )  # use semicolon as , also used in parm names
  end

  fm_fn = joinpath( model_outdir, string("results_turing", "_", aulab, "_fm", ".csv" ) )  
  CSV.write( fm_fn,  DataFrame( FM, :auto), delim=";" )  # use semicolon as , also used in parm names


```

---

## Figures

Finally some plots.

```julia
  #  pl = plot(pl, ylim=(0, 2.5))

  if  occursin( r"logistic_discrete", model_variation ) 
  
    # annual snapshots of biomass (kt) 
    pl = fishery_model_plot( toplot=("survey", "fishing" ) )
    # pl = plot(pl, ylim=(0, 4.0))
    savefig(pl, joinpath( model_outdir, string("plot_predictions_", aulab, ".pdf") )  )

    # plot fishing mortality
    pl = fishery_model_plot( toplot="fishing_mortality" )
    # pl = plot(pl, ylim=(0, 0.65))
    savefig(pl, joinpath( model_outdir, string("plot_fishing_mortality_", aulab, ".pdf") )  )

    # HCR plot
    pl = fishery_model_plot( toplot="harvest_control_rule", n_sample=1000 ) #, alphav=0.01 )  # hcr
    # pl = plot(pl, ylim=(0, 0.65))
    savefig(pl, joinpath( model_outdir, string("plot_hcr_", aulab, ".pdf") )  )

  end


  if  occursin( r"size_structured", model_variation ) 
    
    pl = fishery_model_plot( toplot=("survey", "fishing", "nofishing", "trace"), alphav=0.025 )
    # pl = plot(pl, ylim=(0, 1.5))
    savefig(pl, joinpath( model_outdir, string("plot_predictions_everything_", aulab, ".pdf") )  )


    pl = fishery_model_plot( toplot=("trace", "survey"), alphav=0.025 )
    # pl = plot(pl, ylim=(0, 5.5))
    savefig(pl, joinpath( model_outdir, string("plot_predictions_trace_", aulab, ".pdf") )  )

    # annual snapshots of biomass (kt) 
    pl = fishery_model_plot( toplot=("survey", "fishing" ) )
    savefig(pl, joinpath( model_outdir, string("plot_predictions_", aulab, ".pdf") )  )

    # annual snapshots of biomass (kt) 
    pl = fishery_model_plot( toplot=("survey", "fishing", "nofishing") )
    savefig(pl, joinpath( model_outdir, string("plot_predictions_full_", aulab, ".pdf") )  )

    # plot fishing mortality
    pl = fishery_model_plot( toplot="fishing_mortality" )
    # pl = plot!(pl; ylim=(0, 1.6 ) )
    savefig(pl, joinpath( model_outdir, string("plot_fishing_mortality_", aulab, ".pdf") )  )

    # HCR plot
    pl = fishery_model_plot( toplot="harvest_control_rule", n_sample=1000  )  # hcr
   #  pl = plot!(pl; ylim=(0, 1.75 ), xlim=(0,6.5) )
    savefig(pl, joinpath( model_outdir, string("plot_hcr_", aulab, ".pdf") )  )

    # HCR footprint
    pl = fishery_model_plot( toplot="harvest_control_rule_footprint", n_sample=1000  )  # hcr with fishing footprint
    pl = plot!(pl; ylim=(0, 1.0 ) )
    savefig(pl, joinpath( model_outdir, string("plot_hcr_footprint_", aulab, ".pdf") )  )
        
    # fishery footprint
    pl = fishery_model_plot( toplot="footprint" )
    pl = plot!(pl; ylim=(0, 1.0 ) )
    savefig(pl, joinpath( model_outdir, string("plot_footprint_", aulab, ".pdf") )  )

    pl = fishery_model_plot( toplot="trace_footprint", alphav=0.02 )
    pl = plot!(pl; ylim=(0, 1.0 ) )
    savefig(pl, joinpath( model_outdir, string("plot_footprint_trace_", aulab, ".pdf") )  )
    
    pl = fishery_model_plot( toplot="fishing_mortality_vs_footprint" )
    pl = plot!(pl; ylim=(0, 1.0 ) )
    pl = plot!(pl; xlim=(0, 2.0 ) )
    savefig(pl, joinpath( model_outdir, string("plot_fishing_mortality_vs_footprint_", aulab, ".pdf") )  )
 
    # timeseries of predictions (number; kn and pl =plot) -- not relevent if only 1 state varable
    statevar = 1  # index of S
    pl = fishery_model_plot( toplot="number", si=statevar )  # s1 as numbers
    savefig(pl, joinpath( model_outdir, string("plot_predictions_timeseries_",  aulab, "_", statevar, ".pdf") )  )

    statevar = 2  # index of S
    pl = fishery_model_plot( toplot="number", si=statevar )  # s1 as numbers
    savefig(pl, joinpath( model_outdir, string("plot_predictions_timeseries_",  aulab, "_", statevar, ".pdf") )  )

    statevar = 3  # index of S
    pl = fishery_model_plot( toplot="number", si=statevar )  # s1 as numbers
    savefig(pl, joinpath( model_outdir, string("plot_predictions_timeseries_",  aulab, "_", statevar, ".pdf") )  )

    statevar = 4  # index of S
    pl = fishery_model_plot( toplot="number", si=statevar )  # s1 as numbers
    savefig(pl, joinpath( model_outdir, string("plot_predictions_timeseries_",  aulab, "_", statevar, ".pdf") )  )

    statevar = 5  # index of S
    pl = fishery_model_plot( toplot="number", si=statevar )  # s1 as numbers
    savefig(pl, joinpath( model_outdir, string("plot_predictions_timeseries_",  aulab, "_", statevar, ".pdf") )  )
    
    statevar = 6  # index of S
    pl = fishery_model_plot( toplot="number", si=statevar )  # s1 as numbers
    savefig(pl, joinpath( model_outdir, string("plot_predictions_timeseries_",  aulab, "_", statevar, ".pdf") )  )
 
  end
 
  
  if  occursin( r"size_structured", model_variation ) 
      
    # projections relative to status quo .. update the values below 
    status_quo_tac = [0.9799, 7.345, 0.125 ][ki] # 2022
    for frac in (0.8, 0.9, 1.0, 1.1, 1.2)

      Catch = status_quo_tac * frac 
      m, num, bio, trace, trace_bio, trace_time = project_with_constant_catch( 
        res, 
        solver_params=solver_params, 
        PM=PM, 
        Catch=Catch, 
        ny_fishing_pattern=5  
      )
      
      Fkt, FR, FM = fishery_model_mortality() 
      
      pl = fishery_model_plot( 
        toplot=("trace_projections"), alphav=0.025, time_range_predictions=(2002,year_assessment+4)  
      )

      savefig(pl, joinpath( model_outdir, string("plot_trace_projections_", aulab, "__", frac, "__",".pdf") )  )
      
      pl = fishery_model_plot( 
        toplot=("trace_footprint_projections"), alphav=0.025,  time_range_predictions=(2002,year_assessment+4)  
      )
      
      savefig(pl, joinpath( model_outdir, string("plot_trace_footprint_projections_", aulab, "__", frac, "__",".pdf") )  )

    end
  
  end


```

# End
---


~

~

~


---
### Tests of other methods of parameter estimation and notes (IGNORE)

```julia
restart_method = false
if restart_method
  using Optim, StatsBase
  res_map = Optim.optimize( fmod, MAP())  # find starting point (modes from maximum aposteriori)
  coeftable( res_map)
  res = sample( fmod, Turing.NUTS(), 30, init_params=res_map.values.array )
end


do_variational_inference = false
if do_variational_inference

  # to do Variational Inference (an sd term goes less than 0 .. not sure of the cause ):
  
  using DistributionsAD, AdvancedVI

  res_vi =  vi(fmod, Turing.ADVI( 10, 1000));

     # Run turing_sampler, collect results. @doc(Variational.ADVI) :
     # samples_per_step::Int64
     # Number of samples used to estimate the ELBO in each optimization step.
     # max_iters::Int64
     # Maximum number of gradient steps.

  res_vi_samples = rand( res_vi, 1000)  # sample via simulation


  p1 = histogram(res_vi_samples[1, :]; bins=100, normed=true, alpha=0.2, color=:blue, label="")
  density!(res_vi_samples[1, :]; label="s (ADVI)", color=:blue, linewidth=2)
  density!(res, :s; label="s (NUTS)", color=:green, linewidth=2)
  vline!([var(x)]; label="s (data)", color=:black)
  vline!([mean(res_vi_samples[1, :])]; color=:blue, label="")

  p2 = histogram(res_vi_samples[2, :]; bins=100, normed=true, alpha=0.2, color=:blue, label="")
  density!(res_vi_samples[2, :]; label="m (ADVI)", color=:blue, linewidth=2)
  density!(res, :m; label="m (NUTS)", color=:green, linewidth=2)
  vline!([mean(x)]; color=:black, label="m (data)")
  vline!([mean(res_vi_samples[2, :])]; color=:blue, label="")

  plot(p1, p2; layout=(2, 1), size=(900, 500))


  do_maximum_likelihood = false
  if do_maximum_likelihood
    res_mle = Turing.optimize(fmod, MLE())
    res_mle = Turing.optimize(fmod, MLE())
    res_mle = Turing.optimize(fmod, MLE(), NelderMead())
    res_mle = Turing.optimize(fmod, MLE(), SimulatedAnnealing())
    res_mle = Turing.optimize(fmod, MLE(), ParticleSwarm())
    res_mle = Turing.optimize(fmod, MLE(), Newton())
    res_mle = Turing.optimize(fmod, MLE(), AcceleratedGradientDescent())
    res_mle = Turing.optimize(fmod, MLE(), Newton(), Optim.Options(iterations=10_000, allow_f_increases=true))

    using StatsBase
    coeftable(res_mle)
end


do_maximum_aposteriori = false
if do_maximum_aposteriori
  res_map = Turing.optimize(fmod, MAP())
  using StatsBase
  coeftable(res_map)
end


 
rng = MersenneTwister(26)
resp = predict(rng, textmodel_marginal_pred(data), res)
@df resp ecdfplot(:"b5[1]"; label="birth rate 1")



using Flux, DiffEqFlux
params = Flux.params(p)

msol =  solve( prob,  solver, callback=cb, saveat=dt)    # isoutofdomain=(y,p,t)->any(x->x<0,y)  # to force positive

function predict_rd() # Our 1-layer "neural network"
  solve(prob, solver, p=p, saveat=dt)[1] # override with new parameters  # isoutofdomain=(y,p,t)->any(x->x<0,y)  # to force positive
end

loss_rd() = sum(abs2,x-1 for x in predict_rd()) # loss function (squared absolute) x-1

data = Iterators.repeated((), 100)
opt = ADAM(0.1)
cbflux = function () #callback
  # function to observe training
  display(loss_rd())
  # using `remake` to re-create our `prob` with current parameters `p`
  display(plot(solve(remake(prob,p=p), solver,saveat=dt), ylim=(0,kmu*2)))
end

# Display the ODE with the initial parameter values.
cb()

Flux.train!(loss_rd, params, data, opt, cb = cbflux)

m = Chain(
  Conv((2,2), 1=>16, relu),
  x -> maxpool(x, (2,2)),
  Conv((2,2), 16=>8, relu),
  x -> maxpool(x, (2,2)),
  x -> reshape(x, :, size(x, 4)),
  x -> solve(prob, solver,u0=x,saveat=0.1)[1,:],
  Dense(288, 10), softmax) |> gpu

```



      
 
